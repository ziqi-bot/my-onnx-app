import React, { useEffect, useRef, useState, useCallback } from 'react';
import { InferenceSession, Tensor, env } from 'onnxruntime-web';
import cv from "@techstark/opencv-js";
import './YOLO.css';

const classNames = [
    "pedestrian",
    "biker",
    "skater",
    "cart",
    "car",
    "bus"
];

const COLORS = [
    [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125], [0.494, 0.184, 0.556], 
    [0.466, 0.674, 0.188], [0.301, 0.745, 0.933], [0.635, 0.078, 0.184], [0.300, 0.300, 0.300], 
    [0.600, 0.600, 0.600], [1.000, 0.000, 0.000], [1.000, 0.500, 0.000], [0.749, 0.749, 0.000], 
    [0.000, 1.000, 0.000], [0.000, 0.000, 1.000], [0.667, 0.000, 1.000], [0.333, 0.333, 0.000], 
    [0.333, 0.667, 0.000], [0.333, 1.000, 0.000], [0.667, 0.333, 0.000], [0.667, 0.667, 0.000], 
    [0.667, 1.000, 0.000], [1.000, 0.333, 0.000], [1.000, 0.667, 0.000], [1.000, 1.000, 0.000], 
    [0.333, 0.500, 0.000], [0.667, 0.500, 0.000], [1.000, 0.500, 0.000], [0.500, 0.333, 0.000], 
    [0.500, 0.667, 0.000], [0.500, 1.000, 0.000], [0.333, 0.333, 0.333], [0.500, 0.500, 0.500], 
    [0.667, 0.667, 0.667], [0.833, 0.833, 0.833], [1.000, 1.000, 1.000]
];

interface YOLOProps {
  videoFile: File;
  onProcessComplete: (params: ProcessCompleteParams) => void;
}

interface ProcessCompleteParams {
  videoUrl: string;
  detections: string[];
}

const YOLO: React.FC<YOLOProps> = ({ videoFile, onProcessComplete }) => {
    const canvasRef = useRef<HTMLCanvasElement>(null);
    const videoRef = useRef<HTMLVideoElement>(null);
    const [session, setSession] = useState<InferenceSession | null>(null);
    const [processedVideoUrl, setProcessedVideoUrl] = useState<string | null>(null);

    useEffect(() => {
        const checkOpenCVLoaded = () => {
            if (window.cv && window.cv.Mat) {
                loadModel();
            } else {
                setTimeout(checkOpenCVLoaded, 100);
            }
        };

        checkOpenCVLoaded();
    }, []);

    const loadModel = async () => {
        console.log("Loading model...");
        try {
            env.wasm.numThreads = 4;
            const session = await InferenceSession.create('./yolov4-tiny-obj.onnx');
            console.log("Model loaded successfully.");
            setSession(session);
        } catch (error) {
            console.error("Error loading model:", error);
        }
    };

    const preprocess = (frame: cv.Mat) => {
        console.log('Frame:', frame);
        const inputSize = new cv.Size(416, 416);
        const resized = new cv.Mat();
        cv.resize(frame, resized, inputSize, 0, 0, cv.INTER_LINEAR);
        console.log('Resized:', resized);

        const matC3 = new cv.Mat();
        cv.cvtColor(resized, matC3, cv.COLOR_RGBA2RGB);

        const inputBlob = cv.blobFromImage(matC3, 1 / 255.0, inputSize, new cv.Scalar(0, 0, 0), true, false);
        resized.delete();
        matC3.delete();
        return inputBlob;
    };

    const getDetections = (output: Tensor, scoreThreshold: number, imageWidth: number, imageHeight: number): [number[][], number[], number[]] => {
        const predictions = output.data as Float32Array;
        const numDetections = predictions.length / 10;
        console.log(numDetections)
        const boxes: number[][] = [];
        const scores: number[] = [];
        const classIndices: number[] = [];

        for (let i = 0; i < numDetections; i++) {
            const offset = i * 10;
            const [x1, y1, x2, y2, ...clsScores] = Array.from(predictions.slice(offset, offset + 10));
            const maxClsScore = Math.max(...clsScores);
            const score =  maxClsScore;

            if (score > scoreThreshold) {
                const xMin = x1 * imageWidth;
                const yMin = y1 * imageHeight;
                const xMax = x2 * imageWidth;
                const yMax = y2 * imageHeight;

                boxes.push([xMin, yMin, xMax, yMax]);
                scores.push(score);
                classIndices.push(clsScores.indexOf(maxClsScore));
            }
        }

        return [boxes, scores, classIndices];
    };

    const processVideo = useCallback(async (videoElement: HTMLVideoElement) => {
        if (!session) {
            console.error("Session is not defined.");
            return;
        }

        console.log("Processing video...");
        await videoElement.play();

        const canvas = canvasRef.current;
        const ctx = canvas?.getContext('2d');

        if (canvas && ctx) {
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;

            const detectFrame = async () => {
                console.log("Detecting frame...");
                if (videoElement.paused || videoElement.ended) {
                    const videoBlob = new Blob([canvas.toDataURL('image/png')], { type: 'video/mp4' });
                    const videoUrl = URL.createObjectURL(videoBlob);
                    setProcessedVideoUrl(videoUrl);
                    onProcessComplete({
                        videoUrl: videoUrl,
                        detections: [] // Fill in detection results here
                    });
                    return;
                }

                ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

                const mat = cv.matFromImageData(imageData);
                console.log('Is mat an instance of Mat?', mat instanceof cv.Mat);
                const input = preprocess(mat);
                mat.delete();

                const tensor = new Tensor('float32', input.data32F, [1, 3, 416, 416]);
                console.log("Input tensor:", tensor);
                const feeds = { input: tensor };
                console.log("Feeds:", feeds);

                try {
                    const output = await session.run(feeds);
                    console.log("Inference output:", output);

                    const [boxes, scores, classIndices] = getDetections(output['output'], 0.2, canvas.width, canvas.height);
                    console.log("Detections:", { boxes, scores, classIndices });
                    if (boxes.length > 0) {
                        drawBoxes(ctx, boxes, scores, classIndices);
                    }
                     // 确保 canvas 显示
                canvas.style.display = 'block';
                videoElement.style.display = 'none';

                    requestAnimationFrame(detectFrame);
                } catch (error) {
                    console.error("Error during inference:", error);
                }
            };

            detectFrame(); // Start processing frames
        }
    }, [session, onProcessComplete]);

    useEffect(() => {
        if (videoFile && session) {
            console.log("Video file and session are ready.");
            const videoElement = document.createElement('video');
            videoElement.src = URL.createObjectURL(videoFile);

            videoElement.onloadedmetadata = () => {
                console.log("Video metadata loaded.");
                processVideo(videoElement);
            };

            videoElement.onerror = () => {
                console.error("Error loading video metadata.");
            };
        }
    }, [videoFile, session, processVideo]);

    const drawBoxes = (ctx: CanvasRenderingContext2D, boxes: number[][], scores: number[], classIndices: number[]) => {
        boxes.forEach((box, i) => {
          const [x0, y0, x1, y1] = box;
          const color = COLORS[classIndices[i]];
      
          // 使用更深的颜色
          const r = Math.min(color[0] * 255 + 50, 255);
          const g = Math.min(color[1] * 255 + 50, 255);
          const b = Math.min(color[2] * 255 + 50, 255);
      
          ctx.strokeStyle = `rgb(${r}, ${g}, ${b})`;
          ctx.lineWidth = 4; // 边界框更粗
      
          ctx.strokeRect(x0, y0, x1 - x0, y1 - y0);
      
          const text = `${classNames[classIndices[i]]}: ${scores[i].toFixed(1)}`;
          ctx.font = "16px Arial"; // 标签字体更大
          ctx.fillStyle = `rgba(${r}, ${g}, ${b}, 0.1)`; // 使用更深的颜色
          ctx.fillRect(x0, y0 - 25, x1 - x0, 25);
        //   ctx.fillRect(x0, y0 - 25, ctx.measureText(text).width + 10, 25);
          ctx.fillStyle = "rgb(0, 0, 0)"; // 标签文本为黑色
          ctx.fillText(text, x0, y0 - 5);
        });
      };

    return (
        <div className="video-container">
      <canvas ref={canvasRef}></canvas>
    </div>
    );
};

export default YOLO;













import React, { useEffect, useRef, useState, useCallback } from 'react';
import { InferenceSession, Tensor, env } from 'onnxruntime-web';
import cv from "@techstark/opencv-js";
import './YOLO.css';

const classNames = [
    "pedestrian",
    "biker",
    "skater",
    "cart",
    "car",
    "bus"
];

const COLORS = [
    [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125], [0.494, 0.184, 0.556], 
    [0.466, 0.674, 0.188], [0.301, 0.745, 0.933], [0.635, 0.078, 0.184], [0.300, 0.300, 0.300], 
    [0.600, 0.600, 0.600], [1.000, 0.000, 0.000], [1.000, 0.500, 0.000], [0.749, 0.749, 0.000], 
    [0.000, 1.000, 0.000], [0.000, 0.000, 1.000], [0.667, 0.000, 1.000], [0.333, 0.333, 0.000], 
    [0.333, 0.667, 0.000], [0.333, 1.000, 0.000], [0.667, 0.333, 0.000], [0.667, 0.667, 0.000], 
    [0.667, 1.000, 0.000], [1.000, 0.333, 0.000], [1.000, 0.667, 0.000], [1.000, 1.000, 0.000], 
    [0.333, 0.500, 0.000], [0.667, 0.500, 0.000], [1.000, 0.500, 0.000], [0.500, 0.333, 0.000], 
    [0.500, 0.667, 0.000], [0.500, 1.000, 0.000], [0.333, 0.333, 0.333], [0.500, 0.500, 0.500], 
    [0.667, 0.667, 0.667], [0.833, 0.833, 0.833], [1.000, 1.000, 1.000]
];

interface YOLOProps {
  videoFile: File;
  onProcessComplete: (params: ProcessCompleteParams) => void;
}

interface ProcessCompleteParams {
  videoUrl: string;
  detections: string[];
}

const YOLO: React.FC<YOLOProps> = ({ videoFile, onProcessComplete }) => {
    const canvasRef = useRef<HTMLCanvasElement>(null);
    const videoRef = useRef<HTMLVideoElement>(null);
    const [session, setSession] = useState<InferenceSession | null>(null);
    const [processedVideoUrl, setProcessedVideoUrl] = useState<string | null>(null);

    useEffect(() => {
        const checkOpenCVLoaded = () => {
            if (window.cv && window.cv.Mat) {
                loadModel();
            } else {
                setTimeout(checkOpenCVLoaded, 100);
            }
        };

        checkOpenCVLoaded();
    }, []);

    const loadModel = async () => {
        console.log("Loading model...");
        try {
            env.wasm.numThreads = 4;
            const session = await InferenceSession.create('./yolov4-tiny-obj.onnx');
            console.log("Model loaded successfully.");
            setSession(session);
        } catch (error) {
            console.error("Error loading model:", error);
        }
    };

    const preprocess = (frame: cv.Mat) => {
        console.log('Frame:', frame);
        const inputSize = new cv.Size(416, 416);
        const resized = new cv.Mat();
        cv.resize(frame, resized, inputSize, 0, 0, cv.INTER_LINEAR);
        console.log('Resized:', resized);

        const matC3 = new cv.Mat();
        cv.cvtColor(resized, matC3, cv.COLOR_RGBA2RGB);

        const inputBlob = cv.blobFromImage(matC3, 1 / 255.0, inputSize, new cv.Scalar(0, 0, 0), true, false);
        resized.delete();
        matC3.delete();
        return inputBlob;
    };

    const nms = (boxes: number[][], scores: number[], nmsThreshold: number): number[] => {
        const x1 = boxes.map(box => box[0]);
        const y1 = boxes.map(box => box[1]);
        const x2 = boxes.map(box => box[2]);
        const y2 = boxes.map(box => box[3]);
        const areas = boxes.map((_, i) => (x2[i] - x1[i]) * (y2[i] - y1[i]));
        let order = scores.map((score, i) => [score, i]).sort((a, b) => b[0] - a[0]).map(([_, i]) => i);
        const keep: number[] = [];
      
        while (order.length > 0) {
          const i = order.shift()!;
          keep.push(i);
      
          order = order.filter(j => {
            const xx1 = Math.max(x1[i], x1[j]);
            const yy1 = Math.max(y1[i], y1[j]);
            const xx2 = Math.min(x2[i], x2[j]);
            const yy2 = Math.min(y2[i], y2[j]);
            const w = Math.max(0, xx2 - xx1);
            const h = Math.max(0, yy2 - yy1);
            const inter = w * h;
            const ovr = inter / (areas[i] + areas[j] - inter);
            return ovr <= nmsThreshold;
          });
        }
      
        return keep;
      };
      

    const getDetections = (output: Tensor, scoreThreshold: number, imageWidth: number, imageHeight: number): [number[][], number[], number[]] => {
        const predictions = output.data as Float32Array;
        const numDetections = predictions.length / 10;
        console.log(numDetections)
        const boxes: number[][] = [];
        const scores: number[] = [];
        const classIndices: number[] = [];

        for (let i = 0; i < numDetections; i++) {
            const offset = i * 10;
            const [x1, y1, x2, y2, ...clsScores] = Array.from(predictions.slice(offset, offset + 10));
            const maxClsScore = Math.max(...clsScores);
            const score =  maxClsScore;

            if (score > scoreThreshold) {
                const xMin = x1 * imageWidth;
                const yMin = y1 * imageHeight;
                const xMax = x2 * imageWidth;
                const yMax = y2 * imageHeight;

                boxes.push([xMin, yMin, xMax, yMax]);
                scores.push(score);
                classIndices.push(clsScores.indexOf(maxClsScore));
            }
        }

        // Apply NMS
  const keep = nms(boxes, scores, 0.45);

  return [keep.map(i => boxes[i]), keep.map(i => scores[i]), keep.map(i => classIndices[i])];
};

    const processVideo = useCallback(async (videoElement: HTMLVideoElement) => {
        if (!session) {
            console.error("Session is not defined.");
            return;
        }

        console.log("Processing video...");
        await videoElement.play();

        const canvas = canvasRef.current;
        const ctx = canvas?.getContext('2d');

        if (canvas && ctx) {
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;

            const detectFrame = async () => {
                console.log("Detecting frame...");
                if (videoElement.paused || videoElement.ended) {
                    const videoBlob = new Blob([canvas.toDataURL('image/png')], { type: 'video/mp4' });
                    const videoUrl = URL.createObjectURL(videoBlob);
                    setProcessedVideoUrl(videoUrl);
                    onProcessComplete({
                        videoUrl: videoUrl,
                        detections: [] // Fill in detection results here
                    });
                    return;
                }

                ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

                const mat = cv.matFromImageData(imageData);
                console.log('Is mat an instance of Mat?', mat instanceof cv.Mat);
                const input = preprocess(mat);
                mat.delete();

                const tensor = new Tensor('float32', input.data32F, [1, 3, 416, 416]);
                console.log("Input tensor:", tensor);
                const feeds = { input: tensor };
                console.log("Feeds:", feeds);

                try {
                    const output = await session.run(feeds);
                    console.log("Inference output:", output);

                    const [boxes, scores, classIndices] = getDetections(output['output'], 0.2, canvas.width, canvas.height);
                    console.log("Detections:", { boxes, scores, classIndices });
                    if (boxes.length > 0) {
                        drawBoxes(ctx, boxes, scores, classIndices);
                    }
                     // 确保 canvas 显示
                canvas.style.display = 'block';
                videoElement.style.display = 'none';

                    requestAnimationFrame(detectFrame);
                } catch (error) {
                    console.error("Error during inference:", error);
                }
            };

            detectFrame(); // Start processing frames
        }
    }, [session, onProcessComplete]);

    useEffect(() => {
        if (videoFile && session) {
            console.log("Video file and session are ready.");
            const videoElement = document.createElement('video');
            videoElement.src = URL.createObjectURL(videoFile);

            videoElement.onloadedmetadata = () => {
                console.log("Video metadata loaded.");
                processVideo(videoElement);
            };

            videoElement.onerror = () => {
                console.error("Error loading video metadata.");
            };
        }
    }, [videoFile, session, processVideo]);

    const drawBoxes = (ctx: CanvasRenderingContext2D, boxes: number[][], scores: number[], classIndices: number[]) => {
        boxes.forEach((box, i) => {
          const [x0, y0, x1, y1] = box;
          const color = COLORS[classIndices[i]];
      
          // 使用更深的颜色
          const r = Math.min(color[0] * 255 + 50, 255);
          const g = Math.min(color[1] * 255 + 50, 255);
          const b = Math.min(color[2] * 255 + 50, 255);
      
          ctx.strokeStyle = `rgb(${r}, ${g}, ${b})`;
          ctx.lineWidth = 4; // 边界框更粗
      
          ctx.strokeRect(x0, y0, x1 - x0, y1 - y0);
      
          const text = `${classNames[classIndices[i]]}: ${scores[i].toFixed(1)}`;
          ctx.font = "16px Arial"; // 标签字体更大
          ctx.fillStyle = `rgba(${r}, ${g}, ${b}, 0.1)`; // 使用更深的颜色
          ctx.fillRect(x0, y0 - 25, x1 - x0, 25);
        //   ctx.fillRect(x0, y0 - 25, ctx.measureText(text).width + 10, 25);
          ctx.fillStyle = "rgb(0, 0, 0)"; // 标签文本为黑色
          ctx.fillText(text, x0, y0 - 5);
        });
      };

    return (
        <div className="video-container">
      <canvas ref={canvasRef}></canvas>
    </div>
    );
};

export default YOLO;
